{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " u'i',\n",
       " u'\\u2019',\n",
       " u'm',\n",
       " u'sure',\n",
       " u'_i_',\n",
       " u'shan',\n",
       " u'\\u2019',\n",
       " u't',\n",
       " u'be',\n",
       " u'able',\n",
       " '</s>']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from itertools import permutations\n",
    "\n",
    "f = open('../../data/alice_in_wonderland.txt', 'r')\n",
    "\n",
    "text = f.read().decode('utf-8').lower()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "noOfSentences = len(sentences)\n",
    "\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "v= []\n",
    "stopwords = ['!', ',', ';', '?', '.', '-', '!', '*']\n",
    "for token in tokens:\n",
    "    if token not in stopwords:\n",
    "        v.append(token)\n",
    "\n",
    "\n",
    "vocabulary = list(set(v))\n",
    "v_ = [ nltk.word_tokenize(sentences[i]) for i in xrange(noOfSentences) ]\n",
    "\n",
    "\n",
    "\n",
    "l = []\n",
    "\n",
    "for i in xrange(noOfSentences):\n",
    "    sent = v_[i]\n",
    "    r = ['<s>']\n",
    "    for j in xrange(len(sent)):\n",
    "        if sent[j] not in stopwords:\n",
    "            r.append(sent[j])\n",
    "    r.append('</s>')\n",
    "    l.append(r)\n",
    "    \n",
    "# we take these many sentences because computing them would be time consuming, however it should not \n",
    "# affect the algorithm of the model\n",
    "sentences = l[:100]\n",
    "noOfSentences = len(sentences)\n",
    "\n",
    "\n",
    "\n",
    "trainingData_no = int(noOfSentences*0.8)\n",
    "\n",
    "trainingData = [] #trainData\n",
    "testingData = [] #testData\n",
    "\n",
    "random.shuffle(sentences)\n",
    "\n",
    "continous_text = []\n",
    "for i in xrange(trainingData_no):\n",
    "    trainingData.append(sentences[i])\n",
    "    continous_text.extend(sentences[i])\n",
    "for i in xrange(trainingData_no, noOfSentences):\n",
    "    testingData.append(sentences[i])\n",
    "\n",
    "'''\n",
    "continous text in helping count n-grams\n",
    "'''\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "All sentences here are given as input in the form of a list, where the list starts with '<s> and\n",
    "ends with '</s>'\n",
    "\n",
    "These tags too are important for our probability count and are included in our bigram and trigram count\n",
    "'''\n",
    "\n",
    "\n",
    "class language_model(object):\n",
    "    def __init__(self, continous_text, vocabulary):\n",
    "        self.continous_text = continous_text\n",
    "        self.tokens = len(self.continous_text)\n",
    "        self.vocabulary = vocabulary\n",
    "        self.vocabulary_size = len(vocabulary)\n",
    "        self.len_continous_text = len(continous_text)\n",
    "        self.unigrams = []\n",
    "        self.no_of_unigrams = 0\n",
    "        self.unigrams_p =[]\n",
    "        self.bigrams = []\n",
    "        self.no_of_bigrams = 0\n",
    "        self.bigrams_p = []\n",
    "        self.trigrams = []\n",
    "        self.no_of_trigrams = 0\n",
    "        self.trigrams_p = []\n",
    "        self.quadgrams = []\n",
    "        self.no_of_quadgrams = 0\n",
    "        self.quadgrams_p = []\n",
    "        self.unigrams_count = [0]*10000\n",
    "        self.bigrams_count = [0]*10000\n",
    "        self.count_flag = 0\n",
    "\n",
    "    def find_word_set(self, *args):\n",
    "        count = 0\n",
    "        arg_l = len(args)\n",
    "        for i in xrange(self.len_continous_text-arg_l+1):\n",
    "            t = 0\n",
    "            while t < arg_l:\n",
    "                if args[t] == self.continous_text[i+t]:\n",
    "                    if t == arg_l-1:\n",
    "                        count += 1\n",
    "                else:\n",
    "                    break\n",
    "                t += 1\n",
    "        return float(count)\n",
    "\n",
    "    def unigram_count_cal(self):\n",
    "        for i in xrange(self.vocabulary_size):\n",
    "            count = self.find_word_set(self.vocabulary[i])\n",
    "            self.unigrams_count[int(count)] += 1\n",
    "\n",
    "    def bigram_count_cal(self):\n",
    "        for i in xrange(self.vocabulary_size):\n",
    "            for j in xrange(self.vocabulary_size):\n",
    "                count = self.find_word_set(self.vocabulary[i], self.vocabulary[j])\n",
    "                self.bigrams_count[int(count)] += 1\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "# n - gram models return probability, [original count of the n-gram ( denominator ) and the pseudocount] , discount\n",
    "\n",
    "# trigram and quadgram do not have the \"gt\" smoothing in them yet\"\n",
    "# Work in progress\n",
    "\n",
    "    def unigram_model(self, smoothing,  word):\n",
    "        f =  self.find_word_set(word)\n",
    "        count_before = f\n",
    "        if f == 0:\n",
    "            count_before = 1\n",
    "        if smoothing == \"add1\":\n",
    "            count_after = (f+1)/(self.tokens + self.vocabulary_size)*self.tokens\n",
    "            return (f+1)/(self.tokens + self.vocabulary_size), [count_before, count_after], count_after/count_before\n",
    "        if smoothing == \"gt\":\n",
    "            count_after = ((f+1)*self.unigrams_count[f+1]/self.unigrams_count[f])\n",
    "            return self.unigrams_count[f+1]/self.tokens, [count_before, count_after], count_after/count_before\n",
    "        if smoothing == None:\n",
    "            return f/self.tokens, [count_before, count_before], 1\n",
    "\n",
    "    def bigram_model(self, smoothing, word_1, word_2):\n",
    "        c1 = self.find_word_set(word_1, word_2)\n",
    "        c2 = self.find_word_set(word_2)\n",
    "        count_before = c1\n",
    "        if c1 == 0:\n",
    "            count_before = 0.00001\n",
    "        if smoothing == \"add1\":\n",
    "            count_after = (c1+1)/(c2+self.vocabulary_size)*c2\n",
    "            return (c1+1)/(c2+self.vocabulary_size), [count_before, count_after], count_after/count_before\n",
    "        if smoothing == \"gt\":\n",
    "            if self.bigrams_count[int(c1+1)] == 0:\n",
    "                self.bigrams_count[int(c1+1)] = 1\n",
    "            count_after = ((c1+1)*self.bigrams_count[int(c1+1)]/self.bigrams_count[int(c1+1)])\n",
    "            if c2 == 0:\n",
    "                c2 =  0.00001\n",
    "            return self.bigrams_count[int(c1+1)]/c2, [count_before, count_after], count_after/count_before\n",
    "        if smoothing == None:\n",
    "            if c2 == 0:\n",
    "                return 0, [0,0], 0\n",
    "            else:\n",
    "                return c1/c2 , [c1, c1], 1\n",
    "\n",
    "    def trigram_model(self, smoothing, word_1, word_2, word_3):\n",
    "        c1 = self.find_word_set(word_1, word_2, word_3)\n",
    "        c2 = self.find_word_set(word_1, word_2)\n",
    "        count_before = c1\n",
    "        if c1== 0:\n",
    "            count_before = 0.00001\n",
    "        if smoothing == \"add1\":\n",
    "            count_after = (c1+1)/(c2+self.vocabulary_size)*c2\n",
    "            return (c1+1)/(c2+self.vocabulary_size), [count_before, count_after], count_after/count_before\n",
    "        if smoothing == \"gt\":\n",
    "            print \"still in the making\"\n",
    "        if smoothing == None:\n",
    "            if c2 == 0:\n",
    "                return 0, [0,0], 0\n",
    "            else:\n",
    "                return c1/c2 , [c1, c1], 1\n",
    "\n",
    "    def quadgram_model(self, smoothing, word_1, word_2, word_3, word_4):\n",
    "        c1 = self.find_word_set(word_1, word_2, word_3, word_4)\n",
    "        c2 = self.find_word_set(word_1, word_2, word_3)\n",
    "        count_before = c1\n",
    "        if c1== 0:\n",
    "            count_before =  0.00001\n",
    "        if smoothing == \"add1\":\n",
    "            count_after = (c1+1)/(c2+self.vocabulary_size)*c2\n",
    "            return (c1+1)/(c2+self.vocabulary_size) , [count_before, count_after], count_after/count_before\n",
    "        if smoothing == \"gt\":\n",
    "            print \"still in the making\"\n",
    "        if smoothing == None:\n",
    "            if c2 == 0:\n",
    "                return 0, [0,0], 0\n",
    "            else:\n",
    "                return c1/c2 , [c1, c1], 1\n",
    "\n",
    "        \n",
    "    def n_gram_count_all(self):\n",
    "        for i in xrange(self.vocabulary_size):\n",
    "            p1, c1, d = self.unigram_model(None, self.vocabulary[i])\n",
    "            self.unigrams_p.append(p1)\n",
    "            self.unigrams.append([self.vocabulary[i]])\n",
    "            if c1[0] != 0:\n",
    "                self.no_of_unigrams += 1\n",
    "            for j in xrange(self.vocabulary_size):\n",
    "                p2, c2, d = self.bigram_model(None, self.vocabulary[i], self.vocabulary[j])\n",
    "                self.bigrams_p.append(p2)\n",
    "                self.bigrams.append([self.vocabulary[i], self.vocabulary[j]])\n",
    "                self.bigrams_count[int(c2[0])] += 1\n",
    "                if c2[0] != 0:\n",
    "                    self.no_of_bigrams += 1\n",
    "                for k in xrange(self.vocabulary_size):\n",
    "                    p3, c3, d = self.trigram_model(None, self.vocabulary[i], self.vocabulary[j], self.vocabulary[k])\n",
    "                    self.trigrams_p.append(p3)\n",
    "                    self.trigrams.append([self.vocabulary[i], self.vocabulary[j], self.vocabulary[k]])\n",
    "                    if c3[0] != 0:\n",
    "                        self.no_of_trigrams += 1\n",
    "                    for l in xrange(self.vocabulary_size):\n",
    "                        p4, c4, d = self.quadgram_model(None, self.vocabulary[i], self.vocabulary[j], self.vocabulary[k], self.vocabulary[l])\n",
    "                        self.quadgrams_p.append(p4)\n",
    "                        self.quadgrams.append([self.vocabulary[i], self.vocabulary[j], self.vocabulary[k], self.vocabulary[l]])\n",
    "                        if c4[0] != 0:\n",
    "                            self.no_of_quadgrams += 1\n",
    "        \n",
    "    \n",
    "    def n_gram_count_exist(self):\n",
    "        for i in xrange(self.vocabulary_size):\n",
    "            p1, c1, d = self.unigram_model(None, self.vocabulary[i])\n",
    "            self.unigrams_p.append(p1)\n",
    "            self.unigrams.append([self.vocabulary[i]])\n",
    "            self.no_of_unigrams += 1\n",
    "        for i in xrange(self.vocabulary_size-1):\n",
    "            p2, c2, d = self.bigram_model(None, self.vocabulary[i], self.vocabulary[i+1])\n",
    "            self.bigrams_p.append(p2)\n",
    "            self.bigrams.append([self.vocabulary[i], self.vocabulary[i+1]])\n",
    "            self.bigrams_count[int(c2[0])] += 1\n",
    "            self.no_of_bigrams += 1\n",
    "        for i in xrange(self.vocabulary_size-2):\n",
    "            p3, c3, d = self.trigram_model(None, self.vocabulary[i], self.vocabulary[i+1], self.vocabulary[i+2])\n",
    "            self.trigrams_p.append(p3)\n",
    "            self.trigrams.append([self.vocabulary[i], self.vocabulary[i+1], self.vocabulary[i+2]])\n",
    "            self.no_of_trigrams += 1\n",
    "        for i in xrange(self.vocabulary_size-3):\n",
    "            p4, c4, d = self.quadgram_model(None, self.vocabulary[i], self.vocabulary[i+1], self.vocabulary[i+2], self.vocabulary[i+3])\n",
    "            self.quadgrams_p.append(p4)\n",
    "            self.quadgrams.append([self.vocabulary[i], self.vocabulary[i+1], self.vocabulary[i+2], self.vocabulary[i+3]])\n",
    "            self.no_of_quadgrams += 1\n",
    "\n",
    "\n",
    "    \n",
    "class sentence_model(object):\n",
    "    def __init__(self, continous_text, vocabulary, language_model):\n",
    "        self.continous_text = continous_text\n",
    "        self.vocabulary = vocabulary\n",
    "        #self.language_model = language_model\n",
    "        self.trainingmodel = language_model(self.continous_text, self.vocabulary)\n",
    "\n",
    "    def sentence_prob(self, sentence, smoothing, model):\n",
    "        l = 0\n",
    "        if model == \"unigram\":\n",
    "            l = 1\n",
    "            for i in xrange(len(sentence)):\n",
    "                p,c,d =  self.trainingmodel.unigram_model(smoothing, sentence[i])\n",
    "                l *= p\n",
    "        if model == \"bigram\":\n",
    "            p,c,d = self.trainingmodel.unigram_model(smoothing, sentence[0])\n",
    "            l = p\n",
    "            for i in xrange(len(sentence)-1):\n",
    "                p,c,d = self.trainingmodel.bigram_model(smoothing, sentence[i], sentence[i+1])\n",
    "                l *= p\n",
    "        if model == \"trigram\":\n",
    "            p1, c1, d1 = self.trainingmodel.unigram_model(smoothing, sentence[0])\n",
    "            p2, c2, d2 =  self.trainingmodel.bigram_model(smoothing, sentence[0], sentence[1])\n",
    "            l = p1*p2\n",
    "            for i in xrange(len(sentence)-2):\n",
    "                p,c,d = self.trainingmodel.trigram_model(smoothing, sentence[i], sentence[i+1], sentence[i+2])\n",
    "                l *= p\n",
    "        if model == \"quadgram\":\n",
    "            p1, c1, d1 = self.trainingmodel.unigram_model(smoothing, sentence[0])\n",
    "            p2, c2, d2 = self.trainingmodel.bigram_model(smoothing, sentence[0], sentence[1]) \n",
    "            p3, c3, d3 = self.trainingmodel.trigram_model(smoothing, sentence[0], sentence[1], sentence[2])\n",
    "            l = p1*p2*p3\n",
    "            for i in xrange(len(sentence)-3):\n",
    "                p,c,d = self.trainingmodel.quadgram_model(smoothing, sentence[i], sentence[i+1], sentence[i+2], sentence[i+3])\n",
    "                l *= p\n",
    "        if l == 0:\n",
    "            l = 0.00001\n",
    "        return math.log(l, 10)\n",
    "\n",
    "    def sentence_generator(self, model, smoothing):\n",
    "        if self.trainingmodel.count_flag == 0:\n",
    "            self.trainingmodel.n_gram_count_exist()\n",
    "            self.trainingmodel.count_flag = 1\n",
    "        r = ['<s>']\n",
    "        if model == \"unigram\" : \n",
    "            while r[-1] != '</s>' and len(r) < 10:\n",
    "                t = np.random.multinomial(100, self.trainingmodel.unigrams_p, size=1)\n",
    "                index = t.argmax()\n",
    "                r.append(self.trainingmodel.unigrams[index])\n",
    "        if model == \"bigram\":\n",
    "            while r[-1] != '</s>' and len(r) < 10:\n",
    "                p_list = []\n",
    "                for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                    p,c,d = self.trainingmodel.bigram_model(smoothing, r[-1],self.trainingmodel.vocabulary[i])\n",
    "                    p_list.append(p)\n",
    "                l = sum(p_list)\n",
    "                if l == 0:\n",
    "                    l = 0.00001\n",
    "                for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p_list[i] = p_list[i]/l\n",
    "                t = np.random.multinomial(100, p_list, size=1)\n",
    "                index = t.argmax()\n",
    "                r.append(self.trainingmodel.vocabulary[index])\n",
    "        if model == \"trigram\":\n",
    "            count = 0 \n",
    "            while r[-1] != '</s>' and len(r) < 10:\n",
    "                if count == 0:\n",
    "                    p_list = []\n",
    "                    for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p,c,d = self.trainingmodel.bigram_model(smoothing, r[-1],self.trainingmodel.vocabulary[i])\n",
    "                        p_list.append(p)\n",
    "                    l = sum(p_list)\n",
    "                    if l == 0:\n",
    "                        l = 0.00001\n",
    "                    for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p_list[i] = p_list[i]/l\n",
    "                    t = np.random.multinomial(100, p_list, size=1)\n",
    "                    index = t.argmax()\n",
    "                    r.append(self.trainingmodel.vocabulary[index])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    p_list = []\n",
    "                    for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p,c,d = self.trainingmodel.trigram_model(smoothing, r[-2], r[-1], self.trainingmodel.vocabulary[i])\n",
    "                        p_list.append(p)\n",
    "                    l = sum(p_list)\n",
    "                    if l == 0:\n",
    "                        l = 0.00001\n",
    "                    for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p_list[i] = p_list[i]/l\n",
    "                    t = np.random.multinomial(100, p_list, size=1)\n",
    "                    index = t.argmax()\n",
    "                    r.append(self.trainingmodel.vocabulary[index])\n",
    "        if model == \"quadgram\":\n",
    "            count = 0\n",
    "            while r[-1] != '</s>' and len(r) < 10:\n",
    "                if count == 0:\n",
    "                    p_list = []\n",
    "                    for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p,c,d = self.trainingmodel.bigram_model(smoothing, r[-1],self.trainingmodel.vocabulary[i])\n",
    "                        p_list.append(p)\n",
    "                    l = sum(p_list)\n",
    "                    if l == 0:\n",
    "                        l = 0.00001\n",
    "                    for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p_list[i] = p_list[i]/l\n",
    "                    t = np.random.multinomial(100, p_list, size=1)\n",
    "                    index = t.argmax()\n",
    "                    r.append(self.trainingmodel.vocabulary[index])\n",
    "                    count += 1\n",
    "                elif count == 1:\n",
    "                    p_list = []\n",
    "                    for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p,c,d = self.trainingmodel.trigram_model(smoothing, r[-2], r[-1], self.trainingmodel.vocabulary[i])\n",
    "                        p_list.append(p)\n",
    "                    l = sum(p_list)\n",
    "                    if l == 0:\n",
    "                        l = 0.00001\n",
    "                    for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p_list[i] = p_list[i]/l\n",
    "                    t = np.random.multinomial(100, p_list, size=1)\n",
    "                    index = t.argmax()\n",
    "                    r.append(self.trainingmodel.vocabulary[index])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    p_list = []\n",
    "                    for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p,c,d = self.trainingmodel.quadgram_model(smoothing, r[-3], r[-2], r[-1], self.trainingmodel.vocabulary[i])\n",
    "                        p_list.append(p)\n",
    "                    l = sum(p_list)\n",
    "                    if l == 0:\n",
    "                        l = 0.00001\n",
    "                    for i in xrange(self.trainingmodel.vocabulary_size):\n",
    "                        p_list[i] = p_list[i]/l\n",
    "                    t = np.random.multinomial(100, p_list, size=1)\n",
    "                    index = t.argmax()\n",
    "                    r.append(self.trainingmodel.vocabulary[index])\n",
    "                    count += 1\n",
    "        return r\n",
    "\n",
    "\n",
    "    def bigram_perplexity(self, sentence, smoothing):\n",
    "        n = len(sentence)\n",
    "        #prob, c =  trainingmodel.unigram_model(smoothing, sentence[0])\n",
    "        p = 1\n",
    "        for i in xrange(n-1):\n",
    "            prob, c,d = self.trainingmodel.bigram_model(smoothing, sentence[i], sentence[i+1])\n",
    "            if prob != 0:\n",
    "                p *= prob\n",
    "        return (1.0/p)**(1.0/n)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# we initialise our language models\n",
    "\n",
    "lang_model = language_model(continous_text, vocabulary)\n",
    "sent_model = sentence_model(continous_text, vocabulary, language_model)\n",
    "\n",
    "testingData[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000344471236652\n"
     ]
    }
   ],
   "source": [
    "#MLE for different n-grams\n",
    "\n",
    "prob, c, dis = lang_model.unigram_model(None, 'afraid')\n",
    "print prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0588235294118\n"
     ]
    }
   ],
   "source": [
    "prob, c, dis = lang_model.bigram_model(None, 'as', 'this')\n",
    "print prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "prob, c, dis = lang_model.trigram_model(None, 'you', 'ever', 'eat')\n",
    "print prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "prob, c, dis = lang_model.quadgram_model(None, 'you', 'ever', 'eat', 'this')\n",
    "print prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3257 3256 3255 3254\n"
     ]
    }
   ],
   "source": [
    "lang_model.n_gram_count_exist()\n",
    "# PLEASE NOTE THE FOLLOWING NUMBERS ARE CONSIDERING THE LIMITED DATASET WE USED\n",
    "print \"number of unigrams, bigrams, trigrams and quadgrams which are actually there\"\n",
    "print lang_model.no_of_unigrams, lang_model.no_of_bigrams, lang_model.no_of_trigrams, lang_model.no_of_quadgrams\n",
    "# the number of bigrams which are possible = n-p-2 \n",
    "# the number of trigrams = n-p-3\n",
    "# the number of quadgrams = n-p-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible number of bigrams 10598280\n",
      "possible number of trigrams 34455027810\n",
      "possible number of quadgrams 111910057350024\n"
     ]
    }
   ],
   "source": [
    "print \"possible number of bigrams\", math.factorial(lang_model.no_of_bigrams)/math.factorial(lang_model.no_of_bigrams-2)\n",
    "print \"possible number of trigrams\", math.factorial(lang_model.no_of_trigrams)/math.factorial(lang_model.no_of_trigrams-3)\n",
    "print \"possible number of quadgrams\", math.factorial(lang_model.no_of_quadgrams)/math.factorial(lang_model.no_of_quadgrams-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.0\n",
      "-5.0\n"
     ]
    }
   ],
   "source": [
    "#sentence probability measurements\n",
    "\n",
    "testingData[7]\n",
    "print sent_model.sentence_prob(testingData[7],None, \"bigram\")\n",
    "print sent_model.sentence_prob(testingData[7], None, \"quadgram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', [u'the'], [u'indirectly'], [u'the'], [u'it'], [u'indirectly'], [u'and'], [u'she'], [u'indirectly'], [u'the']]\n",
      "['<s>', u'presently', u'she', u'ate', u'a', u'bat', u'and', u'picking', u'the', u'simple']\n",
      "['<s>', u'however', u'on', u'the', u'second', u'time', u'round', u'she', u'came', u'upon']\n",
      "['<s>', u'london', u'is', u'the', u'capital', u'of', u'paris', u'and', u'paris', u'is']\n",
      "['<s>', [u'indirectly'], [u'indirectly'], [u'she'], [u'\\u2019'], [u'indirectly'], [u'\\u2019'], [u'indirectly'], [u'indirectly'], [u'\\u2019']]\n"
     ]
    }
   ],
   "source": [
    "print sent_model.sentence_generator(\"unigram\", None)\n",
    "print sent_model.sentence_generator(\"bigram\", None)\n",
    "print sent_model.sentence_generator(\"trigram\", None)\n",
    "print sent_model.sentence_generator(\"quadgram\", None)\n",
    "print sent_model.sentence_generator(\"unigram\", \"add1\")\n",
    "\n",
    "#sentence generation using various models and one with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000597193191998 0.0108695652174\n",
      "count before and after [1.0, 0.05494177366378024]\n",
      "0.000895789787996 0.0217391304348\n",
      "count before and after [2.0, 0.08241266049567035]\n",
      "0.000613685179503 0.5\n",
      "count before and after [1.0, 0.00122737035900583]\n"
     ]
    }
   ],
   "source": [
    "bigram_examples = [[u'presently', u'she'], [u'she',u'\\u2019'], [u'table', u'doesn']]\n",
    "for i in bigram_examples:\n",
    "    p1,c1,d1 = lang_model.bigram_model(\"add1\", i[0], i[1])\n",
    "    p2,c2,d2 = lang_model.bigram_model(None, i[0], i[1])\n",
    "    print p1, p2\n",
    "    print 'count before and after', c1\n",
    "    \n",
    "#the following shows the drawback of add1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "bigram_examples = [[u'presently', u'she'], [u'table', u'doesn']]\n",
    "for i in bigram_examples:\n",
    "    p,c,d = lang_model.bigram_model(\"gt\", i[0], i[1])\n",
    "    print d\n",
    "    \n",
    "# the following shows the consistent discounting value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', u'thump', '</s>']\n",
      "175.829779034\n",
      "4.30886938006\n"
     ]
    }
   ],
   "source": [
    "print testingData[2]\n",
    "print sent_model.bigram_perplexity(testingData[2], \"add1\")\n",
    "print sent_model.bigram_perplexity(testingData[2], \"gt\")\n",
    "\n",
    "#Thus Good Turing Smoothing does better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
